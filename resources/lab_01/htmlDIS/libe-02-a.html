
<html><head><title>The Size of MDP Factored
Policies</title></head>
<body>
<hr>
<h2>The Size of MDP Factored Policies</h2>
<b><A HREF=http://www.dis.uniroma1.it/~liberato/ TARGET=_parent>Paolo Liberatore</A></b><p>
<i>Proceedings of the Eighteenth National Conference on
Artificial Intelligence (AAAI 2002)</i><p>
Policies of Markov Decision Processes (MDPs) tell the next action
to execute, given the current state and (possibly) the history of
actions executed so far.   Factorization is used when the number
of states is exponentially large:   both the MDP and the policy
can be then represented using a compact form, for example
employing circuits.   We prove that there are MDPs whose optimal
policies require exponential space even in factored form.
<p><hr>
<PRE>
@inproceedings{libe-02-a,
title = {The Size of MDP Factored Policies},
year = {2002},
author = {Liberatore, Paolo},
booktitle = {Proceedings of the Eighteenth National Conference on
Artificial Intelligence (AAAI 2002)},
pages = {267--272},
}
</PRE>
</body></html>



